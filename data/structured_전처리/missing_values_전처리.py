from lightgbm import LGBMRegressor, LGBMClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import make_pipeline
from tqdm import tqdm
import pandas as pd
import numpy as np
import warnings
import sklearn
from packaging import version
from sklearn.metrics import mutual_info_score
warnings.filterwarnings("ignore")
# 1. ë°ì´í„° ë¡œë“œ
df = pd.read_csv('250530_output.csv')



# 2. ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
missing_cols = df.columns[df.isnull().any()].tolist()

# 3. ë²„ì „ í˜¸í™˜ OneHotEncoder
if version.parse(sklearn.__version__) >= version.parse("1.2"):
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
else:
    encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)

# 4. ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
for i, target_col in enumerate(tqdm(missing_cols, desc="ğŸ”§ ì „ì²´ ì§„í–‰ë¥ ")):
    print(f"\n[{i+1}/{len(missing_cols)}] â¡ï¸ {target_col} ì²˜ë¦¬ ì¤‘...")

    sub_tasks = ["ë°ì´í„° ë¶„ë¦¬", "ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±", "ëª¨ë¸ ìƒì„±", "ëª¨ë¸ í•™ìŠµ", "ì˜ˆì¸¡ ë° ì±„ìš°ê¸°"]
    for step in tqdm(sub_tasks, desc=f"âš™ï¸ {target_col} ì²˜ë¦¬ ë‹¨ê³„", leave=False):
        try:
            if step == "ë°ì´í„° ë¶„ë¦¬":
                # 1) ê²°ì¸¡ì¹˜ ì—†ëŠ” ì—°ì†í˜• ë³€ìˆ˜ë§Œ ì¶”ì¶œí•˜ì—¬ ìƒê´€ê´€ê³„ ê³„ì‚°
                numeric_cols = df.select_dtypes(include=["float64", "int64"]).columns
                corr_candidates = [col for col in numeric_cols if col != target_col and df[col].isnull().sum() == 0]
                
                if len(corr_candidates) == 0:
                    print(f"âš ï¸ {target_col} â†’ ìƒê´€ê´€ê³„ ê³„ì‚° ê°€ëŠ¥í•œ ì—°ì†í˜• ë³€ìˆ˜ ì—†ìŒ")
                    break

                # ìƒê´€ê³„ìˆ˜ ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ top 20 ì¶”ì¶œ
                corrs = df[corr_candidates].corrwith(df[target_col]).abs().sort_values(ascending=False)
                top20_cols = corrs.head(20).index.tolist()

                feature_cols = top20_cols.copy()

                if len(feature_cols) == 0:
                    print(f"âš ï¸ {target_col} â†’ usable feature ì—†ìŒ (ê±´ë„ˆëœ€)")
                    break

                # 2) ë°ì´í„° íƒ€ì…ë³„ë¡œ ë‚˜ëˆ„ê¸°
                cat_cols = [col for col in feature_cols if df[col].dtype == 'object' or df[col].dtype.name == 'category']
                num_cols = [col for col in feature_cols if df[col].dtype in ['int64', 'float64']]

                train_df = df[df[target_col].notna()]
                X_train = train_df[feature_cols]
                y_train = train_df[target_col]
                X_pred = df[df[target_col].isna()][feature_cols]
                pred_idx = df[df[target_col].isna()].index

            elif step == "ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ êµ¬ì„±":
                preprocessor = ColumnTransformer([
                    ('cat', encoder, cat_cols),
                    ('num', 'passthrough', num_cols)
                ])

            elif step == "ëª¨ë¸ ìƒì„±":
                if df[target_col].dtype in ['float64', 'int64']:
                    model = make_pipeline(
                        preprocessor,
                        LGBMRegressor(
                            random_state=42,
                            device="gpu",
                            n_estimators=100,
                            verbose=-1
                        )
                    )
                else:
                    model = make_pipeline(
                        preprocessor,
                        LGBMClassifier(
                            random_state=42,
                            device="gpu",
                            n_estimators=100,
                            verbose=-1
                        )
                    )

            elif step == "ëª¨ë¸ í•™ìŠµ":
                model.fit(X_train, y_train)

            elif step == "ì˜ˆì¸¡ ë° ì±„ìš°ê¸°":
                y_pred = model.predict(X_pred)

                # ì—°ì†í˜• ë³€ìˆ˜ëŠ” ë°˜ì˜¬ë¦¼
                if df[target_col].dtype in ['float64', 'int64']:
                    y_pred = np.round(y_pred)

                df.loc[pred_idx, target_col] = y_pred
                print(f"âœ… {target_col} ê²°ì¸¡ì¹˜ {len(pred_idx)}ê°œ ì±„ì›€")

        except Exception as e:
            print(f"âŒ {target_col} - ë‹¨ê³„ '{step}' ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            break
df.to_csv("final_df_250530.csv", index=False)
print("\nğŸ’¾ ì €ì¥ ì™„ë£Œ: final_df_filled.csv")



import pandas as pd
import numpy as np
from dython.nominal import theils_u
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning)

# ë°ì´í„° ë¡œë“œ
df = pd.read_csv("final_df_250530.csv")

# ì œê±°í•  ì—´ ë¦¬ìŠ¤íŠ¸
erase = [
    'jobwave', 'version', 'hhid26_x', 'hmem26_x', 'hwaveent',
    'p260104', 'p260105', 'p260106', 'p263401'
]

# ì œê±° ìˆ˜í–‰ (ì¡´ì¬í•  ë•Œë§Œ)
df = df.drop(columns=[col for col in erase if col in df.columns])

# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¦¬ìŠ¤íŠ¸
numeric_features = [
    'h260150', 'h260301', 'h260302', 'h260303', 'h260304', 'h260741', 'h260742', 'h260743', 'h260744',
    'h260761', 'h260762', 'h260763', 'h260764', 'h261302', 'h261314', 'h261315', 'h261408', 'h261409',
    'h261410', 'h261411', 'h261412', 'h261413', 'h261414', 'h262102', 'h262134', 'h262153', 'h262153a',
    'h262158', 'h262202', 'h262210', 'h262301', 'h262311', 'h262312', 'h262313', 'h262314', 'h262315',
    'h262316', 'h262317', 'h262318', 'h262319', 'h262320', 'h262321', 'h262323', 'h262324', 'h262325',
    'h262326', 'h262327', 'h262328', 'h262329', 'h262332', 'h262330', 'h262331', 'h262402', 'h262412',
    'h262416', 'h262425', 'h262562', 'h262566', 'h262602', 'h262603', 'h262653', 'j205', 'j206',
    'j730', 'j316', 'j318', 'j322', 'j325', 'j609', 'j610', 'p260107', 'p260114', 'p260115', 'p260301',
    'p260302', 'p260303', 'p260402', 'p260405', 'p260407', 'p261006', 'p261007', 'p261031', 'p261032',
    'p261642', 'p261643', 'p261662', 'p261692', 'p261693', 'p261672', 'p261702', 'p261703', 'p262143',
    'p262152', 'p262153', 'p262154', 'p262160', 'p264160', 'p264161', 'p264164', 'p264172', 'p264414',
    'p265155', 'p265156', 'p265157', 'p265908', 'p265909', 'p265910', 'p265911', 'pa265908', 'pa265909',
    'pa265910', 'pa265911', 'pa269501', 'pa269502', 'pa269507', 'pa269508', 'w26p_l', 'w26p_c',
    'sw26p_l', 'sw26p_c', 'nw26p_l', 'nw26p_c', 'version_y'
]

def normalized_mi(x, y):
    mi = mutual_info_score(x, y)
    h_x = mutual_info_score(x, x)
    h_y = mutual_info_score(y, y)
    return mi / max(h_x, h_y) if max(h_x, h_y) != 0 else 0.0

# ğŸ“Š ìƒê´€ê´€ê³„ ê³„ì‚° í•¨ìˆ˜
def separate_and_compute_correlations(df: pd.DataFrame, numeric_features: list):
    numeric_cols = [col for col in numeric_features if col in df.columns]
    categorical_cols = [col for col in df.columns if col not in numeric_cols]

    # Pearson correlation
    print("ğŸ“ˆ Pearson correlation ê³„ì‚° ì¤‘...")
    numeric_corr = pd.DataFrame(index=numeric_cols, columns=numeric_cols)
    for col1 in tqdm(numeric_cols, desc="Pearson Rows"):
        for col2 in numeric_cols:
            try:
                corr = df[[col1, col2]].corr(method='pearson').iloc[0, 1]
                numeric_corr.loc[col1, col2] = round(corr, 4)
            except:
                numeric_corr.loc[col1, col2] = None
    numeric_corr = numeric_corr.astype(float)

    # Mutual Information (normalized) for categorical vars
    print("ğŸ” Mutual Information ê³„ì‚° ì¤‘...")
    categorical_corr = pd.DataFrame(index=categorical_cols, columns=categorical_cols)
    for col1 in tqdm(categorical_cols, desc="Mutual Info Rows"):
        for col2 in categorical_cols:
            if col1 == col2:
                categorical_corr.loc[col1, col2] = 1.0
            else:
                try:
                    score = normalized_mi(df[col1], df[col2])
                    categorical_corr.loc[col1, col2] = round(score, 4)
                except:
                    categorical_corr.loc[col1, col2] = None
    categorical_corr = categorical_corr.astype(float)

    return numeric_cols, categorical_cols, numeric_corr, categorical_corr

# ì‹¤í–‰
num_cols, cat_cols, num_corr, cat_corr = separate_and_compute_correlations(df, numeric_features)

# ì¶œë ¥
print("\nâœ… ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê°„ Pearson ìƒê´€ê´€ê³„:")
print(num_corr)

print("\nâœ… ë²”ì£¼í˜• ë³€ìˆ˜ ê°„ Mutual Information (normalized):")
print(cat_corr)

# ì €ì¥
num_corr.to_csv("pearson_correlation_matrix.csv")
cat_corr.to_csv("mutual_info_correlation_matrix.csv")
print("\nğŸ’¾ ì €ì¥ ì™„ë£Œ:")
print(" - pearson_correlation_matrix.csv")
print(" - mutual_info_correlation_matrix.csv")
